{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariable Linear Regression with pytorch\n",
    "This code is all about Multivariable linear regression which is formed like a linear euqation graph.\n",
    "\n",
    "I focused on the structure of this code and try to memorize this sample code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see same results at every time, use torch.manual_seed(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bd498c4b30>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 80],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 79]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.size())\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1,requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W,b],lr = 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[153.1693],\n",
      "        [184.0156],\n",
      "        [174.7213],\n",
      "        [197.4559],\n",
      "        [146.3445]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1693, 184.0156, 174.7213, 197.4559, 146.3445],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    0/20 hypothesis: tensor([153.1693, 184.0156, 174.7213, 197.4559, 146.3445]) Cost: 10.239079\n",
      "tensor([[153.1702],\n",
      "        [184.0149],\n",
      "        [174.7241],\n",
      "        [197.4554],\n",
      "        [146.3419]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1702, 184.0149, 174.7241, 197.4554, 146.3419],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    1/20 hypothesis: tensor([153.1702, 184.0149, 174.7241, 197.4554, 146.3419]) Cost: 10.229107\n",
      "tensor([[153.1710],\n",
      "        [184.0142],\n",
      "        [174.7269],\n",
      "        [197.4549],\n",
      "        [146.3393]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1710, 184.0142, 174.7269, 197.4549, 146.3393],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    2/20 hypothesis: tensor([153.1710, 184.0142, 174.7269, 197.4549, 146.3393]) Cost: 10.219156\n",
      "tensor([[153.1719],\n",
      "        [184.0135],\n",
      "        [174.7296],\n",
      "        [197.4544],\n",
      "        [146.3367]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1719, 184.0135, 174.7296, 197.4544, 146.3367],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    3/20 hypothesis: tensor([153.1719, 184.0135, 174.7296, 197.4544, 146.3367]) Cost: 10.209208\n",
      "tensor([[153.1727],\n",
      "        [184.0127],\n",
      "        [174.7323],\n",
      "        [197.4539],\n",
      "        [146.3341]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1727, 184.0127, 174.7323, 197.4539, 146.3341],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    4/20 hypothesis: tensor([153.1727, 184.0127, 174.7323, 197.4539, 146.3341]) Cost: 10.199255\n",
      "tensor([[153.1736],\n",
      "        [184.0120],\n",
      "        [174.7351],\n",
      "        [197.4534],\n",
      "        [146.3315]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1736, 184.0120, 174.7351, 197.4534, 146.3315],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    5/20 hypothesis: tensor([153.1736, 184.0120, 174.7351, 197.4534, 146.3315]) Cost: 10.189377\n",
      "tensor([[153.1744],\n",
      "        [184.0113],\n",
      "        [174.7379],\n",
      "        [197.4530],\n",
      "        [146.3289]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1744, 184.0113, 174.7379, 197.4530, 146.3289],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    6/20 hypothesis: tensor([153.1744, 184.0113, 174.7379, 197.4530, 146.3289]) Cost: 10.179449\n",
      "tensor([[153.1753],\n",
      "        [184.0106],\n",
      "        [174.7406],\n",
      "        [197.4525],\n",
      "        [146.3263]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1753, 184.0106, 174.7406, 197.4525, 146.3263],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    7/20 hypothesis: tensor([153.1753, 184.0106, 174.7406, 197.4525, 146.3263]) Cost: 10.169584\n",
      "tensor([[153.1761],\n",
      "        [184.0099],\n",
      "        [174.7433],\n",
      "        [197.4520],\n",
      "        [146.3237]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1761, 184.0099, 174.7433, 197.4520, 146.3237],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    8/20 hypothesis: tensor([153.1761, 184.0099, 174.7433, 197.4520, 146.3237]) Cost: 10.159722\n",
      "tensor([[153.1770],\n",
      "        [184.0091],\n",
      "        [174.7461],\n",
      "        [197.4515],\n",
      "        [146.3211]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1770, 184.0091, 174.7461, 197.4515, 146.3211],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch    9/20 hypothesis: tensor([153.1770, 184.0091, 174.7461, 197.4515, 146.3211]) Cost: 10.149836\n",
      "tensor([[153.1778],\n",
      "        [184.0084],\n",
      "        [174.7488],\n",
      "        [197.4510],\n",
      "        [146.3185]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1778, 184.0084, 174.7488, 197.4510, 146.3185],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   10/20 hypothesis: tensor([153.1778, 184.0084, 174.7488, 197.4510, 146.3185]) Cost: 10.140002\n",
      "tensor([[153.1787],\n",
      "        [184.0077],\n",
      "        [174.7516],\n",
      "        [197.4505],\n",
      "        [146.3159]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1787, 184.0077, 174.7516, 197.4505, 146.3159],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   11/20 hypothesis: tensor([153.1787, 184.0077, 174.7516, 197.4505, 146.3159]) Cost: 10.130177\n",
      "tensor([[153.1795],\n",
      "        [184.0070],\n",
      "        [174.7543],\n",
      "        [197.4500],\n",
      "        [146.3133]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1795, 184.0070, 174.7543, 197.4500, 146.3133],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   12/20 hypothesis: tensor([153.1795, 184.0070, 174.7543, 197.4500, 146.3133]) Cost: 10.120328\n",
      "tensor([[153.1804],\n",
      "        [184.0063],\n",
      "        [174.7570],\n",
      "        [197.4496],\n",
      "        [146.3107]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1804, 184.0063, 174.7570, 197.4496, 146.3107],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   13/20 hypothesis: tensor([153.1804, 184.0063, 174.7570, 197.4496, 146.3107]) Cost: 10.110559\n",
      "tensor([[153.1812],\n",
      "        [184.0056],\n",
      "        [174.7598],\n",
      "        [197.4491],\n",
      "        [146.3081]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1812, 184.0056, 174.7598, 197.4491, 146.3081],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   14/20 hypothesis: tensor([153.1812, 184.0056, 174.7598, 197.4491, 146.3081]) Cost: 10.100706\n",
      "tensor([[153.1820],\n",
      "        [184.0048],\n",
      "        [174.7625],\n",
      "        [197.4486],\n",
      "        [146.3055]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1820, 184.0048, 174.7625, 197.4486, 146.3055],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   15/20 hypothesis: tensor([153.1820, 184.0048, 174.7625, 197.4486, 146.3055]) Cost: 10.090927\n",
      "tensor([[153.1829],\n",
      "        [184.0041],\n",
      "        [174.7652],\n",
      "        [197.4481],\n",
      "        [146.3029]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1829, 184.0041, 174.7652, 197.4481, 146.3029],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   16/20 hypothesis: tensor([153.1829, 184.0041, 174.7652, 197.4481, 146.3029]) Cost: 10.081157\n",
      "tensor([[153.1837],\n",
      "        [184.0034],\n",
      "        [174.7680],\n",
      "        [197.4476],\n",
      "        [146.3003]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1837, 184.0034, 174.7680, 197.4476, 146.3003],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   17/20 hypothesis: tensor([153.1837, 184.0034, 174.7680, 197.4476, 146.3003]) Cost: 10.071404\n",
      "tensor([[153.1846],\n",
      "        [184.0027],\n",
      "        [174.7707],\n",
      "        [197.4472],\n",
      "        [146.2978]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1846, 184.0027, 174.7707, 197.4472, 146.2978],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   18/20 hypothesis: tensor([153.1846, 184.0027, 174.7707, 197.4472, 146.2978]) Cost: 10.061701\n",
      "tensor([[153.1854],\n",
      "        [184.0020],\n",
      "        [174.7734],\n",
      "        [197.4466],\n",
      "        [146.2952]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1854, 184.0020, 174.7734, 197.4466, 146.2952],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   19/20 hypothesis: tensor([153.1854, 184.0020, 174.7734, 197.4466, 146.2952]) Cost: 10.051908\n",
      "tensor([[153.1862],\n",
      "        [184.0013],\n",
      "        [174.7762],\n",
      "        [197.4462],\n",
      "        [146.2926]], grad_fn=<AddBackward0>)\n",
      "tensor([153.1862, 184.0013, 174.7762, 197.4462, 146.2926],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch   20/20 hypothesis: tensor([153.1862, 184.0013, 174.7762, 197.4462, 146.2926]) Cost: 10.042219\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print(hypothesis)\n",
    "    print(hypothesis.squeeze())\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:3.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
