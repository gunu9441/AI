{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_CNN_MNIST_using_cross_validation","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP+/OIcks/lyrO+LTxCHTSy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-YteONl_7L2i"},"source":["# Deep_CNN_MNIST\n","This code below is all about CNN model to classify MNIST datasets. It is demonstrating a deep CNN model code simulation. \n","\n","In this notebook, I use cross-validation to check the performnace in validation set and trianing dataset. To make cross-validation easily, I use F-fold cross validation which is supported by sklearn.\n"]},{"cell_type":"code","metadata":{"id":"ZaGy2NA083im","executionInfo":{"status":"ok","timestamp":1629820395211,"user_tz":-540,"elapsed":1648,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}}},"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets,transforms\n","import torch.nn.init as init\n","from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, ConcatDataset\n","from sklearn.model_selection import KFold"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLC0S5IrIisR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629820399075,"user_tz":-540,"elapsed":346,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}},"outputId":"52f2b9bf-d3cc-4297-b988-4604ca5e9d18"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('device: {}'.format(device))         "],"execution_count":2,"outputs":[{"output_type":"stream","text":["device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"25fkdL6yf5zS","executionInfo":{"status":"ok","timestamp":1629820400630,"user_tz":-540,"elapsed":2,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}}},"source":["torch.manual_seed(777)\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(777)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWg2h-d_D435","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629820401366,"user_tz":-540,"elapsed":5,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}},"outputId":"149a7706-c1ca-42a3-e796-2c359fb5a5c7"},"source":["transform = transforms.Compose([\n","    transforms.ToTensor(), \n","    transforms.Normalize(mean=[0.5],\n","                         std=[0.5])\n","    ])\n","\n","MNIST_train = datasets.MNIST(root='/MNIST', train=True , transform=transform , download=True)\n","MNIST_test  = datasets.MNIST(root='/MNIST', train=False, transform=transform , download=True)\n","\n","print('The length of MNIST train set:',len(MNIST_train))\n","print('The length of MNIST test set :',len(MNIST_test))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The length of MNIST train set: 60000\n","The length of MNIST test set : 10000\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"O41VuYKaJfQ7","executionInfo":{"status":"ok","timestamp":1629820401369,"user_tz":-540,"elapsed":6,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}}},"source":["class ConvNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n","        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32, kernel_size=5, stride=1, padding=2)\n","        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.relu = nn.ReLU()\n","        self.drop1 = nn.Dropout2d(p=0.5)\n","        self.drop2 = nn.Dropout2d(p=0.1)\n","        \n","        self.fc1 = nn.Linear(32 * 7 * 7, 96)\n","        self.fc2 = nn.Linear(96, 10)\n","\n","    def forward(self,x):\n","         # convolution\n","         # x:[64, 1, 28, 28] \n","\n","         # (activation width + 2*padding - kernel_size) / stride + 1 => next width of next activation map\n","         # (28 + 4 - 5) / 1 + 1 = 28 => (28 -2) / 2 + 1 = 14\n","         # [64, 16, 14, 14]\n","         out = self.relu(self.max_pool1(self.conv1(x)))\n","         \n","         # (14 + 2*2 - 5)/ 1 + 1 = 14 => ((14-2)+2*0) / 2 + 1 = 7\n","         # [64, 32, 7, 7]\n","         out = self.relu(self.max_pool2(self.conv2(out)))\n","         \n","         # flatten => convert 4 dimension into 2 dimension([64, 32, 7, 7]->[64, 1568])\n","         # [64, 1568]\n","         out = out.view(out.size(0),-1)\n","         \n","         # fully connected layer\n","         # [64, 96]\n","         out = self.relu(self.fc1(out))\n","         out = self.drop2(out)\n","         \n","         # [64, 10]\n","         out = self.fc2(out)\n","        \n","         # return shape: [64, 10]\n","         return out\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"4a_NzUiAFU9C","executionInfo":{"status":"ok","timestamp":1629820402042,"user_tz":-540,"elapsed":2,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}}},"source":["model = ConvNet()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeS8N0GpgwKJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629820402639,"user_tz":-540,"elapsed":2,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}},"outputId":"5cc0355a-ce5e-4d5c-c91e-e5751ab5aed6"},"source":["criterion = nn.CrossEntropyLoss()\n","\n","# Concat two datasets to make a one big datset\n","dataset = ConcatDataset([MNIST_train, MNIST_test])\n","print(\"The length of total dataset:\", len(dataset))\n","\n","# Hyper parameters\n","num_epochs = 10\n","batch_size = 64\n","k = 10\n","splits = KFold(n_splits = k, shuffle=True, random_state=777)\n","\n","# Store performance per fold\n","fold_performance={}"],"execution_count":7,"outputs":[{"output_type":"stream","text":["The length of total dataset: 70000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hE0d63uWiwb7","executionInfo":{"status":"ok","timestamp":1629820402640,"user_tz":-540,"elapsed":2,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}}},"source":["def train_epoch(model, device, dataloader,criterion, optimizer):\n","    train_loss, train_correct = 0., 0\n","    model.train()\n","    for num, (images, labels) in enumerate(dataloader):\n","        images, labels = images.to(device), labels.to(device)\n","        # images: [64,1,28,28] labels: [64]\n","        # labels are not one-hot encoded. Instead, they are consist of 1-dimentional numbers such as 0, 1, 2,...,9.\n","        output = model(images)\n","        loss=criterion(output,labels)\n","        #print(loss)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # loss is the mean of mini batch(64 datas) cross entropy loss. \n","        # Therefore, we can get approximate total mini batch cross entropy loss by multiply the number of images with loss.\n","        # train_loss = train_loss + loss * the number of images(batch size)\n","        train_loss += loss.item() * images.size(0)\n","\n","        # To get the maximum float value in 10 dimentional vector(output), use torch.max(torch.Tensor, axis)\n","        scores, predictions = torch.max(output,1) \n","\n","        # To get the number of correct predictions\n","        train_correct +=(predictions==labels).sum().item()\n","    return train_loss, train_correct\n","\n","def valid_epoch(model, device, dataloader, criterion):\n","    valid_loss, valid_correct = 0., 0\n","    model.eval()\n","    for num, (images, labels) in enumerate(dataloader):\n","        images, labels = images.to(device), labels.to(device)\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        valid_loss += loss.item()*images.size(0)\n","        scores, predictions = torch.max(output, 1)\n","        valid_correct += (predictions==labels).sum().item()\n","        \n","    return valid_loss, valid_correct"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4G2BCB8N3mCh","executionInfo":{"status":"ok","timestamp":1629822402436,"user_tz":-540,"elapsed":1999314,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}},"outputId":"bc67ab90-ac06-4f4d-ab68-a9995c4b9cf3"},"source":["# for k(=10) times\n","for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n","    print('Fold {}'.format(fold+1))\n","    # Split indexes for two parts, train_idx and val_dix\n","    # By using SubsetRandomSampler, get indexes randomly to get an effect like dataloader attribute shuffle = 'True'\n","    # In summary, ① use KFold to split indexes into two parts, train_idx, val_idx. \n","    # train_idx is used to get train datasets which are mapping for dataset[train_idx] and this is also applied to val_idx.\n","    # ② And then, use SubseRandomSampler to get random train_idx and val_idx.\n","    # ③ Finally, use dataloader to get batch size data which is mapping to dataset[train_idx] and dataset[val_idx]. \n","    # when we use dataloader, we don't use shuffle attribution because SubsetRadomSampler support giving random index function.\n","    # Therefore, it is fine to get data in fixed order.\n","    train_sampler = SubsetRandomSampler(train_idx)   \n","    test_sampler = SubsetRandomSampler(val_idx)      \n","    train_loader = DataLoader(dataset, batch_size = batch_size, sampler = train_sampler) \n","    test_loader = DataLoader(dataset, batch_size = batch_size, sampler=test_sampler)\n","\n","    model = ConvNet().to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=2e-3)\n","\n","    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n","\n","    # Whole data is used in one epoch everytime by using dataloader.\n","    for epoch in range(num_epochs):\n","        train_loss, train_correct = train_epoch(model, device, train_loader, criterion, optimizer)\n","        test_loss, test_correct = valid_epoch(model, device, test_loader, criterion)\n","        # The length of train_loader.sampler is 63000 because of 10-Fold cross validation in the 70000 legnth of dataset which is composed of 7000 validation set and 63000 training set per fold.\n","        # Therefore, the length of test_loader is 7000.\n","        \n","        train_loss = train_loss / len(train_loader.sampler) \n","        train_acc = train_correct / len(train_loader.sampler) * 100\n","        test_loss = test_loss / len(test_loader.sampler)\n","        test_acc = test_correct / len(test_loader.sampler) * 100\n","    \n","        print(\"Epoch:{}/{} AVG Training Loss: {:.4f} AVG Test Loss: {:.3f} AVG Training ACC: {:.3f}% AVG Test ACC: {:.3f}%\".format(\n","           epoch+1, num_epochs, train_loss, test_loss, train_acc, test_acc\n","        ))\n","\n","        history['train_loss'].append(train_loss)\n","        history['test_loss'].append(test_loss)\n","        history['train_acc'].append(train_acc)\n","        history['test_acc'].append(test_acc)\n","    fold_performance['fold{}'.format(fold+1)] = history\n","\n","torch.save(model,'k_cross_CNN.pt') \n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Fold 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:1/10 AVG Training Loss: 0.1500 AVG Test Loss: 0.052 AVG Training ACC: 95.440% AVG Test ACC: 98.386%\n","Epoch:2/10 AVG Training Loss: 0.0507 AVG Test Loss: 0.053 AVG Training ACC: 98.433% AVG Test ACC: 98.400%\n","Epoch:3/10 AVG Training Loss: 0.0370 AVG Test Loss: 0.031 AVG Training ACC: 98.813% AVG Test ACC: 98.886%\n","Epoch:4/10 AVG Training Loss: 0.0281 AVG Test Loss: 0.033 AVG Training ACC: 99.159% AVG Test ACC: 99.100%\n","Epoch:5/10 AVG Training Loss: 0.0248 AVG Test Loss: 0.041 AVG Training ACC: 99.190% AVG Test ACC: 98.857%\n","Epoch:6/10 AVG Training Loss: 0.0226 AVG Test Loss: 0.031 AVG Training ACC: 99.297% AVG Test ACC: 99.171%\n","Epoch:7/10 AVG Training Loss: 0.0186 AVG Test Loss: 0.030 AVG Training ACC: 99.397% AVG Test ACC: 99.314%\n","Epoch:8/10 AVG Training Loss: 0.0173 AVG Test Loss: 0.038 AVG Training ACC: 99.459% AVG Test ACC: 98.986%\n","Epoch:9/10 AVG Training Loss: 0.0158 AVG Test Loss: 0.037 AVG Training ACC: 99.484% AVG Test ACC: 99.114%\n","Epoch:10/10 AVG Training Loss: 0.0149 AVG Test Loss: 0.035 AVG Training ACC: 99.517% AVG Test ACC: 99.229%\n","Fold 2\n","Epoch:1/10 AVG Training Loss: 0.1360 AVG Test Loss: 0.054 AVG Training ACC: 95.827% AVG Test ACC: 98.457%\n","Epoch:2/10 AVG Training Loss: 0.0466 AVG Test Loss: 0.040 AVG Training ACC: 98.525% AVG Test ACC: 98.743%\n","Epoch:3/10 AVG Training Loss: 0.0334 AVG Test Loss: 0.038 AVG Training ACC: 98.952% AVG Test ACC: 98.886%\n","Epoch:4/10 AVG Training Loss: 0.0281 AVG Test Loss: 0.038 AVG Training ACC: 99.094% AVG Test ACC: 98.914%\n","Epoch:5/10 AVG Training Loss: 0.0231 AVG Test Loss: 0.046 AVG Training ACC: 99.240% AVG Test ACC: 98.671%\n","Epoch:6/10 AVG Training Loss: 0.0200 AVG Test Loss: 0.054 AVG Training ACC: 99.351% AVG Test ACC: 98.871%\n","Epoch:7/10 AVG Training Loss: 0.0163 AVG Test Loss: 0.042 AVG Training ACC: 99.429% AVG Test ACC: 98.957%\n","Epoch:8/10 AVG Training Loss: 0.0157 AVG Test Loss: 0.047 AVG Training ACC: 99.479% AVG Test ACC: 98.914%\n","Epoch:9/10 AVG Training Loss: 0.0140 AVG Test Loss: 0.046 AVG Training ACC: 99.562% AVG Test ACC: 98.886%\n","Epoch:10/10 AVG Training Loss: 0.0133 AVG Test Loss: 0.040 AVG Training ACC: 99.563% AVG Test ACC: 99.100%\n","Fold 3\n","Epoch:1/10 AVG Training Loss: 0.1372 AVG Test Loss: 0.053 AVG Training ACC: 95.849% AVG Test ACC: 98.343%\n","Epoch:2/10 AVG Training Loss: 0.0462 AVG Test Loss: 0.034 AVG Training ACC: 98.552% AVG Test ACC: 98.929%\n","Epoch:3/10 AVG Training Loss: 0.0357 AVG Test Loss: 0.034 AVG Training ACC: 98.908% AVG Test ACC: 98.943%\n","Epoch:4/10 AVG Training Loss: 0.0283 AVG Test Loss: 0.046 AVG Training ACC: 99.124% AVG Test ACC: 98.400%\n","Epoch:5/10 AVG Training Loss: 0.0247 AVG Test Loss: 0.038 AVG Training ACC: 99.237% AVG Test ACC: 98.886%\n","Epoch:6/10 AVG Training Loss: 0.0205 AVG Test Loss: 0.036 AVG Training ACC: 99.349% AVG Test ACC: 98.857%\n","Epoch:7/10 AVG Training Loss: 0.0181 AVG Test Loss: 0.031 AVG Training ACC: 99.410% AVG Test ACC: 99.129%\n","Epoch:8/10 AVG Training Loss: 0.0169 AVG Test Loss: 0.030 AVG Training ACC: 99.429% AVG Test ACC: 99.086%\n","Epoch:9/10 AVG Training Loss: 0.0116 AVG Test Loss: 0.041 AVG Training ACC: 99.646% AVG Test ACC: 99.029%\n","Epoch:10/10 AVG Training Loss: 0.0146 AVG Test Loss: 0.037 AVG Training ACC: 99.544% AVG Test ACC: 99.086%\n","Fold 4\n","Epoch:1/10 AVG Training Loss: 0.1388 AVG Test Loss: 0.050 AVG Training ACC: 95.608% AVG Test ACC: 98.471%\n","Epoch:2/10 AVG Training Loss: 0.0461 AVG Test Loss: 0.035 AVG Training ACC: 98.617% AVG Test ACC: 98.900%\n","Epoch:3/10 AVG Training Loss: 0.0328 AVG Test Loss: 0.036 AVG Training ACC: 98.967% AVG Test ACC: 99.043%\n","Epoch:4/10 AVG Training Loss: 0.0274 AVG Test Loss: 0.035 AVG Training ACC: 99.141% AVG Test ACC: 98.900%\n","Epoch:5/10 AVG Training Loss: 0.0230 AVG Test Loss: 0.044 AVG Training ACC: 99.284% AVG Test ACC: 98.614%\n","Epoch:6/10 AVG Training Loss: 0.0181 AVG Test Loss: 0.054 AVG Training ACC: 99.416% AVG Test ACC: 98.686%\n","Epoch:7/10 AVG Training Loss: 0.0177 AVG Test Loss: 0.038 AVG Training ACC: 99.444% AVG Test ACC: 99.071%\n","Epoch:8/10 AVG Training Loss: 0.0147 AVG Test Loss: 0.040 AVG Training ACC: 99.560% AVG Test ACC: 99.171%\n","Epoch:9/10 AVG Training Loss: 0.0145 AVG Test Loss: 0.045 AVG Training ACC: 99.521% AVG Test ACC: 98.886%\n","Epoch:10/10 AVG Training Loss: 0.0132 AVG Test Loss: 0.042 AVG Training ACC: 99.571% AVG Test ACC: 99.343%\n","Fold 5\n","Epoch:1/10 AVG Training Loss: 0.1512 AVG Test Loss: 0.060 AVG Training ACC: 95.251% AVG Test ACC: 98.071%\n","Epoch:2/10 AVG Training Loss: 0.0471 AVG Test Loss: 0.043 AVG Training ACC: 98.537% AVG Test ACC: 98.771%\n","Epoch:3/10 AVG Training Loss: 0.0371 AVG Test Loss: 0.042 AVG Training ACC: 98.894% AVG Test ACC: 98.714%\n","Epoch:4/10 AVG Training Loss: 0.0295 AVG Test Loss: 0.065 AVG Training ACC: 99.054% AVG Test ACC: 98.271%\n","Epoch:5/10 AVG Training Loss: 0.0256 AVG Test Loss: 0.036 AVG Training ACC: 99.163% AVG Test ACC: 99.071%\n","Epoch:6/10 AVG Training Loss: 0.0219 AVG Test Loss: 0.045 AVG Training ACC: 99.321% AVG Test ACC: 98.929%\n","Epoch:7/10 AVG Training Loss: 0.0187 AVG Test Loss: 0.058 AVG Training ACC: 99.413% AVG Test ACC: 98.529%\n","Epoch:8/10 AVG Training Loss: 0.0178 AVG Test Loss: 0.049 AVG Training ACC: 99.430% AVG Test ACC: 98.914%\n","Epoch:9/10 AVG Training Loss: 0.0159 AVG Test Loss: 0.055 AVG Training ACC: 99.521% AVG Test ACC: 98.543%\n","Epoch:10/10 AVG Training Loss: 0.0161 AVG Test Loss: 0.041 AVG Training ACC: 99.463% AVG Test ACC: 98.943%\n","Fold 6\n","Epoch:1/10 AVG Training Loss: 0.1448 AVG Test Loss: 0.051 AVG Training ACC: 95.471% AVG Test ACC: 98.314%\n","Epoch:2/10 AVG Training Loss: 0.0464 AVG Test Loss: 0.037 AVG Training ACC: 98.571% AVG Test ACC: 98.900%\n","Epoch:3/10 AVG Training Loss: 0.0353 AVG Test Loss: 0.041 AVG Training ACC: 98.906% AVG Test ACC: 98.757%\n","Epoch:4/10 AVG Training Loss: 0.0283 AVG Test Loss: 0.048 AVG Training ACC: 99.095% AVG Test ACC: 98.614%\n","Epoch:5/10 AVG Training Loss: 0.0248 AVG Test Loss: 0.033 AVG Training ACC: 99.190% AVG Test ACC: 99.014%\n","Epoch:6/10 AVG Training Loss: 0.0197 AVG Test Loss: 0.041 AVG Training ACC: 99.413% AVG Test ACC: 98.886%\n","Epoch:7/10 AVG Training Loss: 0.0199 AVG Test Loss: 0.028 AVG Training ACC: 99.346% AVG Test ACC: 99.100%\n","Epoch:8/10 AVG Training Loss: 0.0147 AVG Test Loss: 0.034 AVG Training ACC: 99.525% AVG Test ACC: 99.186%\n","Epoch:9/10 AVG Training Loss: 0.0162 AVG Test Loss: 0.025 AVG Training ACC: 99.459% AVG Test ACC: 99.257%\n","Epoch:10/10 AVG Training Loss: 0.0140 AVG Test Loss: 0.028 AVG Training ACC: 99.567% AVG Test ACC: 99.171%\n","Fold 7\n","Epoch:1/10 AVG Training Loss: 0.1523 AVG Test Loss: 0.056 AVG Training ACC: 95.178% AVG Test ACC: 98.143%\n","Epoch:2/10 AVG Training Loss: 0.0495 AVG Test Loss: 0.043 AVG Training ACC: 98.462% AVG Test ACC: 98.629%\n","Epoch:3/10 AVG Training Loss: 0.0383 AVG Test Loss: 0.045 AVG Training ACC: 98.814% AVG Test ACC: 98.700%\n","Epoch:4/10 AVG Training Loss: 0.0298 AVG Test Loss: 0.055 AVG Training ACC: 99.073% AVG Test ACC: 98.457%\n","Epoch:5/10 AVG Training Loss: 0.0255 AVG Test Loss: 0.046 AVG Training ACC: 99.148% AVG Test ACC: 98.814%\n","Epoch:6/10 AVG Training Loss: 0.0232 AVG Test Loss: 0.047 AVG Training ACC: 99.275% AVG Test ACC: 98.857%\n","Epoch:7/10 AVG Training Loss: 0.0203 AVG Test Loss: 0.043 AVG Training ACC: 99.333% AVG Test ACC: 99.114%\n","Epoch:8/10 AVG Training Loss: 0.0170 AVG Test Loss: 0.042 AVG Training ACC: 99.465% AVG Test ACC: 99.029%\n","Epoch:9/10 AVG Training Loss: 0.0181 AVG Test Loss: 0.053 AVG Training ACC: 99.433% AVG Test ACC: 98.757%\n","Epoch:10/10 AVG Training Loss: 0.0146 AVG Test Loss: 0.053 AVG Training ACC: 99.503% AVG Test ACC: 98.800%\n","Fold 8\n","Epoch:1/10 AVG Training Loss: 0.1459 AVG Test Loss: 0.073 AVG Training ACC: 95.490% AVG Test ACC: 97.814%\n","Epoch:2/10 AVG Training Loss: 0.0461 AVG Test Loss: 0.039 AVG Training ACC: 98.587% AVG Test ACC: 98.886%\n","Epoch:3/10 AVG Training Loss: 0.0340 AVG Test Loss: 0.054 AVG Training ACC: 98.911% AVG Test ACC: 98.471%\n","Epoch:4/10 AVG Training Loss: 0.0275 AVG Test Loss: 0.041 AVG Training ACC: 99.124% AVG Test ACC: 98.900%\n","Epoch:5/10 AVG Training Loss: 0.0242 AVG Test Loss: 0.034 AVG Training ACC: 99.224% AVG Test ACC: 98.957%\n","Epoch:6/10 AVG Training Loss: 0.0205 AVG Test Loss: 0.051 AVG Training ACC: 99.349% AVG Test ACC: 98.757%\n","Epoch:7/10 AVG Training Loss: 0.0161 AVG Test Loss: 0.048 AVG Training ACC: 99.497% AVG Test ACC: 98.829%\n","Epoch:8/10 AVG Training Loss: 0.0152 AVG Test Loss: 0.048 AVG Training ACC: 99.537% AVG Test ACC: 98.814%\n","Epoch:9/10 AVG Training Loss: 0.0152 AVG Test Loss: 0.070 AVG Training ACC: 99.556% AVG Test ACC: 98.700%\n","Epoch:10/10 AVG Training Loss: 0.0139 AVG Test Loss: 0.044 AVG Training ACC: 99.557% AVG Test ACC: 98.886%\n","Fold 9\n","Epoch:1/10 AVG Training Loss: 0.1304 AVG Test Loss: 0.050 AVG Training ACC: 95.952% AVG Test ACC: 98.386%\n","Epoch:2/10 AVG Training Loss: 0.0442 AVG Test Loss: 0.040 AVG Training ACC: 98.683% AVG Test ACC: 98.857%\n","Epoch:3/10 AVG Training Loss: 0.0355 AVG Test Loss: 0.040 AVG Training ACC: 98.892% AVG Test ACC: 98.900%\n","Epoch:4/10 AVG Training Loss: 0.0252 AVG Test Loss: 0.050 AVG Training ACC: 99.162% AVG Test ACC: 98.600%\n","Epoch:5/10 AVG Training Loss: 0.0225 AVG Test Loss: 0.035 AVG Training ACC: 99.303% AVG Test ACC: 99.171%\n","Epoch:6/10 AVG Training Loss: 0.0210 AVG Test Loss: 0.036 AVG Training ACC: 99.343% AVG Test ACC: 99.086%\n","Epoch:7/10 AVG Training Loss: 0.0174 AVG Test Loss: 0.037 AVG Training ACC: 99.456% AVG Test ACC: 99.000%\n","Epoch:8/10 AVG Training Loss: 0.0142 AVG Test Loss: 0.037 AVG Training ACC: 99.514% AVG Test ACC: 99.186%\n","Epoch:9/10 AVG Training Loss: 0.0131 AVG Test Loss: 0.049 AVG Training ACC: 99.603% AVG Test ACC: 99.086%\n","Epoch:10/10 AVG Training Loss: 0.0126 AVG Test Loss: 0.052 AVG Training ACC: 99.600% AVG Test ACC: 99.014%\n","Fold 10\n","Epoch:1/10 AVG Training Loss: 0.1323 AVG Test Loss: 0.039 AVG Training ACC: 95.987% AVG Test ACC: 98.757%\n","Epoch:2/10 AVG Training Loss: 0.0467 AVG Test Loss: 0.035 AVG Training ACC: 98.584% AVG Test ACC: 98.986%\n","Epoch:3/10 AVG Training Loss: 0.0333 AVG Test Loss: 0.030 AVG Training ACC: 98.922% AVG Test ACC: 99.071%\n","Epoch:4/10 AVG Training Loss: 0.0274 AVG Test Loss: 0.035 AVG Training ACC: 99.110% AVG Test ACC: 99.043%\n","Epoch:5/10 AVG Training Loss: 0.0218 AVG Test Loss: 0.034 AVG Training ACC: 99.287% AVG Test ACC: 99.071%\n","Epoch:6/10 AVG Training Loss: 0.0188 AVG Test Loss: 0.038 AVG Training ACC: 99.443% AVG Test ACC: 98.900%\n","Epoch:7/10 AVG Training Loss: 0.0156 AVG Test Loss: 0.032 AVG Training ACC: 99.510% AVG Test ACC: 99.286%\n","Epoch:8/10 AVG Training Loss: 0.0172 AVG Test Loss: 0.039 AVG Training ACC: 99.444% AVG Test ACC: 99.014%\n","Epoch:9/10 AVG Training Loss: 0.0119 AVG Test Loss: 0.051 AVG Training ACC: 99.603% AVG Test ACC: 99.071%\n","Epoch:10/10 AVG Training Loss: 0.0144 AVG Test Loss: 0.041 AVG Training ACC: 99.570% AVG Test ACC: 99.029%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZg7biPchyQ0","executionInfo":{"status":"ok","timestamp":1629822402436,"user_tz":-540,"elapsed":20,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}},"outputId":"7cecd3c3-16f2-43d7-cc5b-ee3f0b6e5cc2"},"source":["print(len(fold_performance))\n","\n","for f in range(1,k+1):\n","\n","     print('fold{}'.format(f),len(fold_performance['fold{}'.format(f)]['train_loss']))\n","     print('fold{}'.format(f),len(fold_performance['fold{}'.format(f)]['test_loss']))\n","     print('fold{}'.format(f),len(fold_performance['fold{}'.format(f)]['train_acc']))\n","     print('fold{}'.format(f),len(fold_performance['fold{}'.format(f)]['test_acc']))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["10\n","fold1 10\n","fold1 10\n","fold1 10\n","fold1 10\n","fold2 10\n","fold2 10\n","fold2 10\n","fold2 10\n","fold3 10\n","fold3 10\n","fold3 10\n","fold3 10\n","fold4 10\n","fold4 10\n","fold4 10\n","fold4 10\n","fold5 10\n","fold5 10\n","fold5 10\n","fold5 10\n","fold6 10\n","fold6 10\n","fold6 10\n","fold6 10\n","fold7 10\n","fold7 10\n","fold7 10\n","fold7 10\n","fold8 10\n","fold8 10\n","fold8 10\n","fold8 10\n","fold9 10\n","fold9 10\n","fold9 10\n","fold9 10\n","fold10 10\n","fold10 10\n","fold10 10\n","fold10 10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqz31vTId_uy","executionInfo":{"status":"ok","timestamp":1629822402437,"user_tz":-540,"elapsed":10,"user":{"displayName":"남건우","photoUrl":"","userId":"18221977253473109697"}},"outputId":"a8fb2fb7-cb5a-4688-93b5-1246e06f1fb0"},"source":["testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n","k=10\n","\n","for f in range(1,k+1):\n","\n","     tl_f.append(np.mean(fold_performance['fold{}'.format(f)]['train_loss']))\n","     testl_f.append(np.mean(fold_performance['fold{}'.format(f)]['test_loss']))\n","\n","     ta_f.append(np.mean(fold_performance['fold{}'.format(f)]['train_acc']))\n","     testa_f.append(np.mean(fold_performance['fold{}'.format(f)]['test_acc']))\n","\n","print('Performance of {} fold cross validation'.format(k))\n","print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(\n","    np.mean(tl_f),\n","    np.mean(testl_f),\n","    np.mean(ta_f),\n","    np.mean(testa_f)))     "],"execution_count":11,"outputs":[{"output_type":"stream","text":["Performance of 10 fold cross validation\n","Average Training Loss: 0.036 \t Average Test Loss: 0.043 \t Average Training Acc: 98.87 \t Average Test Acc: 98.85\n"],"name":"stdout"}]}]}